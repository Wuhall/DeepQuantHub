{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57408230",
   "metadata": {},
   "source": [
    "# llama.cpp 量化专题\n",
    "\n",
    "系统了解 llama.cpp 项目、其量化技术原理、应用场景，以及如何在本地高效运行大语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4929ad",
   "metadata": {},
   "source": [
    "## 1. llama.cpp 项目简介\n",
    "\n",
    "llama.cpp 是一个用 C/C++ 实现的高效大语言模型推理引擎，支持 LLaMA、Vicuna、ChatGLM 等模型。它的最大特点是支持极致的低比特量化（如 int8、int4、int3），让大模型能在普通 PC、手机等设备上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb342f6",
   "metadata": {},
   "source": [
    "### 1.1 主要目标\n",
    "- 让大模型本地化推理成为可能，无需高端 GPU。\n",
    "- 通过量化大幅降低模型体积和内存占用。\n",
    "- 利用高效的底层实现和硬件指令集提升推理速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f88905",
   "metadata": {},
   "source": [
    "## 2. llama.cpp 支持的量化方法\n",
    "\n",
    "llama.cpp 支持多种量化格式，包括 int8、int4、int3 等。量化后模型体积可缩小至原始的 1/8~1/3，推理速度和部署门槛大幅降低。\n",
    "\n",
    "常见量化格式：\n",
    "- q8_0, q4_0, q4_1, q2_k, q3_k, q4_k, q5_k, q6_k 等\n",
    "\n",
    "每种格式在精度和速度之间有不同权衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d23a4",
   "metadata": {},
   "source": [
    "## 3. 量化原理简述\n",
    "\n",
    "量化是将模型参数（如权重）从 float32/float16 等高精度类型，映射为低比特整数（如 int8、int4），以减少存储和计算量。\n",
    "\n",
    "核心思想：\n",
    "- 通过 scale/zero_point 将浮点数映射到整数区间\n",
    "- 反量化时再还原为近似的浮点数\n",
    "\n",
    "llama.cpp 采用分组量化、块量化等技术，兼顾精度和效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0caa5f",
   "metadata": {},
   "source": [
    "## 4. 应用场景与优势\n",
    "- 在无 GPU 的 PC、Mac、树莓派等设备本地运行大模型\n",
    "- 移动端、嵌入式设备推理\n",
    "- 私有化部署、边缘计算\n",
    "- 降低推理成本，提升隐私安全"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4314d9",
   "metadata": {},
   "source": [
    "## 5 Hugging Face 模型转换为 llama.cpp 格式的方法\n",
    "\n",
    "llama.cpp 只能加载专用的 .bin 量化权重文件，不能直接加载 Hugging Face 的 safetensors/pt 格式。你需要先将 Hugging Face 格式的模型权重转换为 llama.cpp 支持的 .bin 格式。\n",
    "\n",
    "### 步骤如下：\n",
    "1. 下载原始 Hugging Face 格式的模型（如 model.safetensors、config.json 等）。\n",
    "2. 克隆 llama.cpp 仓库，并进入目录：\n",
    "   ```sh\n",
    "   git clone https://github.com/ggerganov/llama.cpp.git\n",
    "   cd llama.cpp\n",
    "   ```\n",
    "3. 使用 convert.py 脚本将 Hugging Face 权重转换为 llama.cpp 格式（以 Qwen 为例）：\n",
    "   ```sh\n",
    "   python3 convert.py models --outtype f16 --outfile ./qwen2-1.5b-f16.bin /path/to/hf/model/dir\n",
    "   ```\n",
    "   - `--outtype f16` 表示输出为 float16，可选 int8、int4 等。\n",
    "   - `/path/to/hf/model/dir` 是你下载的 Hugging Face 模型文件夹路径。\n",
    "4. 使用 quantize 工具将 f16 权重进一步量化为 int4/int8 等格式（如 q4_0）：\n",
    "   ```sh\n",
    "   ./quantize ./qwen2-1.5b-f16.bin ./qwen2-1.5b-q4_0.bin q4_0\n",
    "   ```\n",
    "5. 得到的 .bin 文件即可用于 llama.cpp 或 llama-cpp-python。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b26b7",
   "metadata": {},
   "source": [
    "### Hugging Face 模型转换为 llama.cpp 格式的代码示例\n",
    "\n",
    "#### 1. 命令行方式（推荐）\n",
    "```sh\n",
    "# 克隆 llama.cpp 仓库\n",
    " git clone https://github.com/ggerganov/llama.cpp.git\n",
    " cd llama.cpp\n",
    "\n",
    "# 使用 convert.py 脚本将 Hugging Face 权重转换为 llama.cpp 格式\n",
    "python3 convert.py models --outtype f16 --outfile ./qwen2-1.5b-f16.bin /path/to/hf/model/dir\n",
    "\n",
    "# 量化为 int4/int8 等格式（如 q4_0）\n",
    "./quantize ./qwen2-1.5b-f16.bin ./qwen2-1.5b-q4_0.bin q4_0\n",
    "```\n",
    "\n",
    "#### 2. Python 脚本自动化示例\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "# 路径根据实际情况修改\n",
    "hf_model_dir = '/path/to/hf/model/dir'\n",
    "llama_cpp_dir = '/path/to/llama.cpp'\n",
    "output_f16 = './qwen2-1.5b-f16.bin'\n",
    "output_q4 = './qwen2-1.5b-q4_0.bin'\n",
    "\n",
    "# 步骤1：转换为f16\n",
    "subprocess.run([\n",
    "    'python3', f'{llama_cpp_dir}/convert.py', 'models',\n",
    "    '--outtype', 'f16',\n",
    "    '--outfile', output_f16,\n",
    "    hf_model_dir\n",
    "])\n",
    "# 步骤2：量化为q4_0\n",
    "subprocess.run([\n",
    "    f'{llama_cpp_dir}/quantize',\n",
    "    output_f16,\n",
    "    output_q4,\n",
    "    'q4_0'\n",
    "])\n",
    "```\n",
    "\n",
    "> 注意：转换和量化过程需要较多内存和磁盘空间，建议在本地PC或服务器上操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee05ae",
   "metadata": {},
   "source": [
    "## 6. 量化模型的推理\n",
    "\n",
    "你可以用 llama.cpp 提供的命令行工具或 Python API 加载量化模型，体验本地推理。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31575954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设已安装 llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 加载量化后的模型（如 q4_0 格式）\n",
    "llm = Llama(model_path=\"./models/llama-7b-q4_0.bin\")\n",
    "\n",
    "# 推理示例\n",
    "output = llm(\"你好，介绍一下llama.cpp的量化原理\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e882a25",
   "metadata": {},
   "source": [
    "> 可以更换不同量化格式的模型文件，体验速度和内存占用的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd3c41",
   "metadata": {},
   "source": [
    "## 7. GGUF 格式简介\n",
    "\n",
    "GGUF（Generalized GGML Unified Format）是 GGML/llama.cpp 生态下新一代的通用大模型权重格式，旨在替代早期的 .bin 格式。\n",
    "\n",
    "- 支持更丰富的模型结构和元数据（如 tokenizer、prompt 模板等）\n",
    "- 兼容 LLaMA、Qwen、Baichuan、Mistral、ChatGLM 等多种主流模型\n",
    "- 便于社区共享和跨平台部署\n",
    "\n",
    "### GGUF 格式的优势\n",
    "- 统一格式，便于模型互操作和工具链集成\n",
    "- 支持多种量化方案（int8/int4/qk等）\n",
    "- 便于后续扩展和维护\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Ollama：一站式本地大模型推理与管理平台\n",
    "\n",
    "Ollama 是一个面向开发者和终端用户的本地 LLM 部署与管理平台，支持一键下载、运行、管理多种主流大模型（如 LLaMA、Qwen、Mistral、Gemma 等），底层基于 GGUF/llama.cpp。\n",
    "\n",
    "### Ollama 的主要特性\n",
    "- 简单易用的命令行和 API\n",
    "- 支持模型热切换、并发推理\n",
    "- 自动管理模型下载、格式转换和量化\n",
    "- 支持自定义模型和微调\n",
    "\n",
    "#### Ollama 快速体验\n",
    "```sh\n",
    "# 安装 Ollama（以 macOS 为例）\n",
    "brew install ollama\n",
    "\n",
    "# 下载并运行 Qwen 模型\n",
    "ollama run qwen:1.5b\n",
    "\n",
    "# 交互式对话\n",
    "ollama run llama2\n",
    "\n",
    "# 列出本地所有模型\n",
    "ollama list\n",
    "```\n",
    "\n",
    "#### Ollama 支持和不支持的模型类型说明\n",
    "\n",
    "> Ollama 适合部署的模型\n",
    "- 主要面向大语言模型（LLM），如 LLaMA、Qwen、Mistral、Gemma、Baichuan、ChatGLM 等。\n",
    "- 要求模型权重为 GGUF 格式（或早期 .bin 格式，推荐 GGUF）。\n",
    "- 适用于生成式任务（对话、文本生成、总结、代码生成等）。\n",
    "- 支持社区主流的 LLM 及其微调版本。\n",
    "\n",
    "> Ollama 不支持的模型类型\n",
    "- 传统的文本分类、情感分析、NER、回归等“非生成式”Transformer模型（如 BERT、RoBERTa、DistilBERT 等）。\n",
    "- 不是大语言模型的 Transformer（如用于分类、序列标注、embedding 等任务）。\n",
    "- Hugging Face Transformers 库训练的标准 PyTorch/TF 模型（如 .pt/.bin/.h5/.safetensors），如果不是 LLM 且未转为 GGUF 格式，Ollama 无法直接加载和推理。\n",
    "- CV、语音、图像等非 NLP 模型。\n",
    "\n",
    "> 文本分类等判别式模型的推荐推理方式\n",
    "- 直接用 transformers 库的 pipeline 或 model.from_pretrained 加载模型推理。\n",
    "- 若需高性能/跨平台，可导出为 ONNX 格式，用 ONNX Runtime 部署。\n",
    "- 对于大规模服务，可用 FastAPI、Flask 等封装 API，或用 TorchServe、Triton Inference Server 等。\n",
    "\n",
    "> 总结：Ollama 只适合大语言模型的生成式推理，不适合文本分类等判别式任务。文本分类模型建议用 transformers、ONNX Runtime、TorchServe 等方式推理和部署。\n",
    "\n",
    "---\n",
    "\n",
    "## 9. LLM 部署方式全景与选型建议\n",
    "\n",
    "### 9.1 主流 LLM 部署方式\n",
    "- 本地推理（llama.cpp/GGUF/Ollama/LMDeploy）\n",
    "- 云端 API（OpenAI、阿里云、百度千帆等）\n",
    "- 私有云/企业内网（vLLM、TGI、FastChat、LMDeploy-Server等）\n",
    "- 混合部署（边缘+云端）\n",
    "\n",
    "### 9.2 选型建议\n",
    "- **个人/开发者本地体验**：Ollama、llama.cpp（GGUF）、LMDeploy\n",
    "- **企业/团队私有化部署**：vLLM、TGI、FastChat、LMDeploy-Server\n",
    "- **高并发/大规模推理**：vLLM、TGI、ONNX Runtime、TensorRT-LLM\n",
    "- **移动端/嵌入式**：llama.cpp、GGML、MNN、NCNN\n",
    "- **云端 API**：OpenAI、阿里云、百度千帆等\n",
    "\n",
    "### 9.3 选型思路\n",
    "- 关注推理速度、内存占用、易用性、生态兼容性\n",
    "- 结合硬件资源（CPU/GPU/内存）和业务需求\n",
    "- 关注社区活跃度和文档支持\n",
    "\n",
    "### 9.4 LLM 部署方式对比\n",
    "\n",
    "| 部署方式         | 典型代表                | 支持格式      | 量化支持 | 推理速度 | 内存占用 | 易用性 | 并发能力 | 适用场景           |\n",
    "|------------------|-------------------------|--------------|----------|----------|----------|--------|----------|--------------------|\n",
    "| 本地 CPU         | llama.cpp, Ollama, LMDeploy | GGUF, bin     | 强      | 中-快    | 低-中    | 高     | 低-中    | 个人/开发机/边缘   |\n",
    "| 本地 GPU         | vLLM, TGI, LMDeploy     | safetensors, GGUF | 可选    | 快-极快  | 中-高    | 中     | 高      | 企业/高并发        |\n",
    "| 云端 API         | OpenAI, 阿里云, 百度千帆 | 专有/多样     | 不限     | 快       | 无需关心 | 极高   | 极高     | 业务集成/快速上线  |\n",
    "| 私有云/内网      | vLLM, TGI, FastChat     | safetensors, GGUF | 可选    | 快-极快  | 中-高    | 中     | 高      | 企业/团队          |\n",
    "| 移动/嵌入式      | llama.cpp, GGML, MNN    | GGUF, bin     | 强      | 中        | 极低     | 中     | 低      | 端侧/IoT           |\n",
    "\n",
    "- 量化支持：本地 CPU/移动端通常强制量化以降低资源消耗，GPU/云端可选。\n",
    "- 推理速度/内存占用：量化模型更优，GPU/云端原生精度更高。\n",
    "- 易用性：Ollama、云端 API 最高，llama.cpp/LMDeploy 也较友好。\n",
    "- 并发能力：云端/GPU方案更强。\n",
    "\n",
    "---\n",
    "\n",
    "### 9.5 GGUF 格式与 Ollama 精度说明\n",
    "\n",
    "- GGUF 是一种通用权重格式，**本身不强制量化**，可以存储 float32、float16、int8、int4 等多种精度的模型。\n",
    "- Ollama 支持加载和运行多种精度的 GGUF 模型，但**Ollama 官方仓库和社区主流模型大多为量化（如 int4/int8）格式**，以便在本地高效运行。\n",
    "- 可以自行导入未量化（float16/float32）或自定义量化精度的 GGUF 模型到 Ollama，但这样会显著增加内存和算力需求。\n",
    "- **结论：Ollama 部署的模型精度主要取决于你加载的 GGUF 文件本身的精度**，不是 Ollama 平台本身决定的。主流社区模型多为量化，精度略低于原生 float16/float32，但可大幅提升推理效率。\n",
    "\n",
    "> 如果对模型精度有极高要求，可以自行转换并加载高精度 GGUF 模型到 Ollama 或 llama.cpp。\n",
    "\n",
    "---\n",
    "\n",
    "## 10. 参考资料与扩展阅读\n",
    "- [llama.cpp 官方文档](https://github.com/ggerganov/llama.cpp)\n",
    "- [GGUF 格式说明](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n",
    "- [Ollama 官网](https://ollama.com/)\n",
    "- [vLLM 项目](https://github.com/vllm-project/vllm)\n",
    "- [TGI 项目](https://github.com/huggingface/text-generation-inference)\n",
    "- [LMDeploy 项目](https://github.com/InternLM/LMDeploy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
