{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c115ab",
   "metadata": {},
   "source": [
    "# 量化（Quantization）\n",
    "\n",
    "量化是指将神经网络中的权重和激活从高精度（如float32）转换为低精度（如int8），以减少模型大小、加速推理并降低能耗。\n",
    "\n",
    "## 1. 量化的基本原理与动机\n",
    "\n",
    "- **减少模型存储和内存占用**\n",
    "- **加速推理速度**\n",
    "- **降低功耗，适合部署在移动端和嵌入式设备**\n",
    "\n",
    "## 2. 量化的主要类型\n",
    "\n",
    "- **权重量化**：只对权重进行量化\n",
    "- **激活量化**：对中间激活值进行量化\n",
    "- **后训练量化（PTQ）**：训练后直接量化\n",
    "- **量化感知训练（QAT）**：训练时模拟量化误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32e4f6",
   "metadata": {},
   "source": [
    "## 3. 权重量化、激活量化的底层实现\n",
    "\n",
    "### (1) 量化过程\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{scale} &= \\frac{\\text{max\\_val} - \\text{min\\_val}}{2^{\\text{num\\_bits}} - 1}, \\\\\n",
    "\\text{zero\\_point} &= \\text{qmin} - \\frac{\\text{min\\_val}}{\\text{scale}}, \\\\\n",
    "q_x &= \\text{round}\\left(\\frac{x}{\\text{scale}} + \\text{zero\\_point}\\right), \\\\\n",
    "q_{\\text{final}} &= \\text{clip}(q_x, \\text{qmin}, \\text{qmax}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### (2) 反量化过程\n",
    "反量化通常在推理结束后需要将量化结果还原为浮点数（如输出展示、误差分析等场景）时用到。\n",
    "\n",
    "$$\n",
    "x_{\\text{dequant}} = \\text{scale} \\times (q_{\\text{final}} - \\text{zero\\_point}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6992de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量: [ 0.46483758  1.1436542  -1.457284   -0.658385    1.2318981   0.6196174\n",
      "  0.95771927  0.00453585 -0.02097602  1.118749  ]\n",
      "量化后: [182 247   0  76 255 197 229 139 136 244]\n",
      "反量化后: [ 0.4620538   1.1475316  -1.4572839  -0.6558022   1.2318981   0.620241\n",
      "  0.957707    0.00858392 -0.02305352  1.1158942 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"量化和反量化\"\"\"\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0 # 量化最小值和最大值，范围为[0, 2^num_bits - 1]\n",
    "    qmax = 2 ** num_bits - 1\n",
    "    min_val, max_val = x.min(), x.max() # 原始张量的最小/最大值\n",
    "    # 计算量化和反量化参数\n",
    "    scale = (max_val - min_val) / (qmax - qmin) # 缩放因子\n",
    "    zero_point = qmin - min_val / scale # 零点偏移\n",
    "    # 量化公式: q_x = round(x / scale + zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = np.clip(np.round(q_x), qmin, qmax) # 限制在 [qmin, qmax] 范围内\n",
    "    return q_x.astype(np.uint8), scale, zero_point # 转换为 uint8 类型\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point):\n",
    "    # 反量化公式: x_dequant = scale * (q_x - zero_point)\n",
    "    return scale * (q_x.astype(np.float32) - zero_point)\n",
    "\n",
    "# 示例：对一个张量进行8位量化和反量化\n",
    "x = np.random.randn(10).astype(np.float32)\n",
    "q_x, scale, zero_point = quantize_tensor(x)\n",
    "x_dequant = dequantize_tensor(q_x, scale, zero_point)\n",
    "\n",
    "print('原始张量:', x)\n",
    "print('量化后:', q_x)\n",
    "print('反量化后:', x_dequant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672b76b",
   "metadata": {},
   "source": [
    "## 4. PyTorch中的后训练量化（PTQ）\n",
    "\n",
    "PyTorch提供了简单的API来对模型进行后训练量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ca8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (fc): QuantizedLinear(in_features=4, out_features=2, scale=0.008065156638622284, zero_point=130, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "torch.qint8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quantization\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'  # 适配 Apple Silicon\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(4, 2) # 定义一个4输入2输出的全连接层\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "model.eval() # 设置为评估模式（不计算梯度）\n",
    "\n",
    "\n",
    "# 设置量化配置\n",
    "model.qconfig = quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# 插入量化/反量化层（准备模型\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# 示例数据，这里通常需要用一批数据跑一遍模型以收集量化参数\n",
    "example_input = torch.randn(10, 4)\n",
    "# 前向传播收集参数\n",
    "model(example_input)\n",
    "# 转换为量化模型\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "print(model)\n",
    "print(model.fc.weight().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e19b60",
   "metadata": {},
   "source": [
    "## 5. PyTorch中的量化感知训练（QAT）\n",
    "\n",
    "QAT在训练过程中模拟量化误差，能获得更高精度的量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f83cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleModel(\n",
      "  (fc): QuantizedLinear(in_features=4, out_features=2, scale=0.010084597393870354, zero_point=141, qscheme=torch.per_tensor_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 量化感知训练示例\n",
    "qat_model = SimpleModel()\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "# 模拟训练过程\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr=0.01)\n",
    "for _ in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = qat_model(torch.randn(10, 4))\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 转换为量化模型\n",
    "torch.quantization.convert(qat_model.eval(), inplace=True)\n",
    "print(qat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75284279",
   "metadata": {},
   "source": [
    "### 6. 量化为何能减少存储/内存占用和加速推理速度？\n",
    "\n",
    "- 低位宽（如int8）权重和激活比float32占用更少的存储空间。\n",
    "- 低精度运算在硬件上通常更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7120252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32权重占用: 3.81 MB\n",
      "int8权重占用: 0.95 MB\n",
      "float32矩阵乘法耗时: 0.0045 秒\n",
      "int8矩阵乘法耗时: 0.3212 秒\n",
      "int8矩阵乘法耗时: 0.3212 秒\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 1. 存储空间对比\n",
    "float32_weights = np.random.randn(1000, 1000).astype(np.float32)\n",
    "int8_weights = (float32_weights / float32_weights.max() * 127).astype(np.int8)\n",
    "\n",
    "print(f\"float32权重占用: {float32_weights.nbytes/1024/1024:.2f} MB\")\n",
    "print(f\"int8权重占用: {int8_weights.nbytes/1024/1024:.2f} MB\")\n",
    "\n",
    "# 2. 推理速度对比（矩阵乘法）\n",
    "x = np.random.randn(1000, 1000).astype(np.float32)\n",
    "\n",
    "start = time.time()\n",
    "_ = np.dot(x, float32_weights)\n",
    "print(f\"float32矩阵乘法耗时: {time.time()-start:.4f} 秒\")\n",
    "\n",
    "x_int8 = (x / x.max() * 127).astype(np.int8)\n",
    "start = time.time()\n",
    "_ = np.dot(x_int8, int8_weights)\n",
    "print(f\"int8矩阵乘法耗时: {time.time()-start:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863f315",
   "metadata": {},
   "source": [
    "### 7. PyTorch在Apple芯片上的float32与int8推理速度对比\n",
    "\n",
    "下面代码对比同一个简单全连接模型在float32和int8（量化后）下的推理速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c498ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch float32模型推理耗时: 0.0251 秒\n",
      "PyTorch int8量化模型推理耗时: 0.0709 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quantization\n",
    "import time\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'  # Apple Silicon推荐\n",
    "\n",
    "# 定义带量化stub的模型\n",
    "class QuantModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = quantization.QuantStub()\n",
    "        self.fc = nn.Linear(1024, 1024)\n",
    "        self.dequant = quantization.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# 生成测试数据\n",
    "x = torch.randn(1000, 1024)\n",
    "\n",
    "# 浮点模型推理\n",
    "float_model = QuantModel().eval()\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = float_model(x)\n",
    "    print(f\"PyTorch float32模型推理耗时: {time.time()-start:.4f} 秒\")\n",
    "\n",
    "# 量化模型推理\n",
    "quant_model = QuantModel().eval()\n",
    "quant_model.qconfig = quantization.get_default_qconfig('qnnpack')\n",
    "quantization.prepare(quant_model, inplace=True)\n",
    "quant_model(x)  # 收集量化参数\n",
    "quantization.convert(quant_model, inplace=True)\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = quant_model(x)\n",
    "    print(f\"PyTorch int8量化模型推理耗时: {time.time()-start:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a6540",
   "metadata": {},
   "source": [
    "**注意：为什么在Mac（Apple Silicon）上int8量化推理反而更慢？**\n",
    "\n",
    "- PyTorch的int8量化推理在CPU上的加速效果，依赖于底层对int8运算的高度优化（如x86 AVX512 VNNI、ARM Neon/SDOT等指令集）。\n",
    "- Apple Silicon（M1/M2/M3）目前PyTorch官方的int8量化后端（qnnpack）对int8没有做专门的硬件加速优化，float32反而能用到高效的BLAS库（如Accelerate）。\n",
    "- 量化模型在推理时还涉及量化/反量化的额外操作，若底层没有专门优化，反而会拖慢速度。\n",
    "\n",
    "**结论：**\n",
    "- 量化的“推理加速”优势主要体现在移动端（如安卓ARM）、云端服务器（如Intel AVX512）、以及专用推理芯片（如NPU、TPU）上。\n",
    "- 在Mac上用PyTorch体验不到int8的速度优势，但模型体积和内存占用依然会显著下降。\n",
    "- 若要体验int8加速，可在x86服务器或使用ONNX Runtime、TensorRT等推理引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bb06f",
   "metadata": {},
   "source": [
    "#### x86平台（如Intel/AMD服务器）PyTorch量化推理加速示例\n",
    "\n",
    "如果你在x86平台（如Intel Xeon/酷睿）上，PyTorch推荐使用'fbgemm'后端，通常能体验到int8推理加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99882865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quantization\n",
    "import time\n",
    "\n",
    "# 仅在x86平台推荐使用'fbgemm'后端\n",
    "if torch.backends.quantized.engine != 'fbgemm':\n",
    "    torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "class QuantModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = quantization.QuantStub()\n",
    "        self.fc = nn.Linear(1024, 120000)\n",
    "        self.dequant = quantization.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(1000, 1024)\n",
    "\n",
    "# 浮点模型推理\n",
    "float_model = QuantModel().eval()\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = float_model(x)\n",
    "    print(f\"[x86] PyTorch float32模型推理耗时: {time.time()-start:.4f} 秒\")\n",
    "\n",
    "# 量化模型推理\n",
    "quant_model = QuantModel().eval()\n",
    "quant_model.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "quantization.prepare(quant_model, inplace=True)\n",
    "quant_model(x)\n",
    "quantization.convert(quant_model, inplace=True)\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = quant_model(x)\n",
    "    print(f\"[x86] PyTorch int8量化模型推理耗时: {time.time()-start:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc6f33",
   "metadata": {},
   "source": [
    "[x86] PyTorch float32模型推理耗时: 35.6941 秒  \n",
    "[x86] PyTorch int8量化模型推理耗时: 26.5078 秒"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477921e0",
   "metadata": {},
   "source": [
    "## 8. 不同量化位宽的存储占用对比\n",
    "\n",
    "常见量化位宽有 float32、float16、int8、int4、int3 等。位宽越低，模型存储和内存占用越小。\n",
    "\n",
    "| 数据类型   | 每元素占用(bit) | 每元素占用(Byte) | 压缩率（相对float32） |\n",
    "|------------|----------------|------------------|----------------------|\n",
    "| float32    | 32             | 4                | 1x                   |\n",
    "| float16    | 16             | 2                | 2x                   |\n",
    "| int8       | 8              | 1                | 4x                   |\n",
    "| int4       | 4              | 0.5              | 8x                   |\n",
    "| int3       | 3              | 0.375            | 10.7x                |\n",
    "| int2       | 2              | 0.25             | 16x                  |\n",
    "\n",
    "- 例如：一个 1GB 的 float32 模型，int8 量化后约为 256MB，int4 量化后约为 128MB。\n",
    "- 实际存储还会有量化参数、对齐等开销，但整体压缩比非常显著。\n",
    "\n",
    "### 代码：不同位宽的权重占用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83956b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据类型      每元素占用(Byte)         模型占用内存（仅权重）                   \n",
      "32        4.0                 32.0                          \n",
      "16        2.0                 16.0                          \n",
      "8         1.0                 8.0                           \n",
      "4         0.5                 4.0                           \n",
      "\n",
      "int8权重示例: tensor([[-0.3581, -0.2374, -0.2452, -0.2725],\n",
      "        [ 0.1713, -0.3386,  0.4943,  0.0000]], size=(2, 4), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0038924638647586107,\n",
      "       zero_point=0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as quantization\n",
    "import numpy as np\n",
    "torch.backends.quantized.engine = 'qnnpack' \n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, num_bits=8):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(4, 2) # 定义一个4输入2输出的全连接层\n",
    "        self.num_bits = num_bits\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 不同位宽的模型\n",
    "models = {bit: SimpleModel(bit).eval() for bit in [32, 16, 8, 4]}\n",
    "# 随机输入\n",
    "x = torch.randn(10, 4)\n",
    "\n",
    "# 存储对比\n",
    "print(f\"{'数据类型':<10}{'每元素占用(Byte)':<20}{'模型占用内存（仅权重）':<30}\")\n",
    "for bit, model in models.items():\n",
    "    # 权重量化\n",
    "    qconfig = quantization.get_default_qconfig('qnnpack')\n",
    "    model.qconfig = qconfig\n",
    "    quantization.prepare(model, inplace=True)\n",
    "    model(x)  # 前向传播收集参数\n",
    "    quantization.convert(model, inplace=True)\n",
    "    \n",
    "    # 计算占用内存\n",
    "    weight_memory = model.fc.weight().numel() * bit / 8\n",
    "    print(f\"{bit:<10}{bit/8:<20}{weight_memory:<30}\")\n",
    "\n",
    "# 示例：查看int8权重\n",
    "model_int8 = models[8]\n",
    "print(\"\\nint8权重示例:\", model_int8.fc.weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027c2b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32占用: 3.81 MB\n",
      "float16占用: 1.91 MB\n",
      "int8占用: 0.95 MB\n",
      "int4占用: 0.48 MB (理论值)\n",
      "int3占用: 0.36 MB (理论值)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设有一个 1000x1000 的权重矩阵\n",
    "shape = (1000, 1000)\n",
    "float32_weights = np.random.randn(*shape).astype(np.float32)\n",
    "float16_weights = float32_weights.astype(np.float16)\n",
    "int8_weights = (float32_weights / np.abs(float32_weights).max() * 127).astype(np.int8)\n",
    "\n",
    "# int4/int3 需要特殊打包，模拟存储空间\n",
    "int4_bytes = float32_weights.size // 2  # 2个int4打包成1字节\n",
    "int3_bytes = int(np.ceil(float32_weights.size * 3 / 8))  # 8个int3占3字节\n",
    "\n",
    "print(f\"float32占用: {float32_weights.nbytes/1024/1024:.2f} MB\")\n",
    "print(f\"float16占用: {float16_weights.nbytes/1024/1024:.2f} MB\")\n",
    "print(f\"int8占用: {int8_weights.nbytes/1024/1024:.2f} MB\")\n",
    "print(f\"int4占用: {int4_bytes/1024/1024:.2f} MB (理论值)\")\n",
    "print(f\"int3占用: {int3_bytes/1024/1024:.2f} MB (理论值)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
